---
title: "p8105_hw2_yg2964"
output: github_document
date: "2024-10-01"
---
Name:Yuchen Gu

```{r,echo=FALSE, message=FALSE}
library(tidyverse)
library(readr)
library(readxl)
library(lubridate)
```
# Problem 1
## Section 1 Import and clean the data
```{r}
nyc_subway_entrances <- read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
  col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) |> 
  janitor::clean_names() |> 
  select(line, station_name, station_latitude, station_longitude, 
    starts_with("route"), entry, vending, entrance_type, ada) |> 
  mutate(entry = case_match(entry, "YES" ~ TRUE, "NO" ~ FALSE, .default = NA))
```

This dataset contains information about NYC subway station entrances and exits. The variables include line, station_name, station_latitude, station_longitude, routes served (multiple columns starting with "route"), entry (whether entry is allowed), vending (presence of vending machines), entrance_type, and ada (ADA compliance). The data cleaning steps involved selecting relevant columns, cleaning column names, and converting the entry variable from character to logical. The resulting dataset has `r nrow(nyc_subway_entrances)` rows and `r ncol(nyc_subway_entrances)` columns. These data are not fully tidy, as the route information is spread across multiple columns rather than being in a single column.

## Section 2 Number of distinct stations
```{r}
n_distinct_stations <- nyc_subway_entrances |> 
  distinct(line,station_name) |> 
  nrow()
```
There are `r n_distinct_stations` distinct stations.

## Section 3 Number of ADA compliant stations
```{r}
n_ada_stations <- nyc_subway_entrances |> 
  filter(ada == TRUE) |> 
  distinct(line, station_name) |> 
  nrow()
```
There are `r n_ada_stations` ADA compliant stations.

## Section 4 Proportion of station entrances/exits without vending that allow entrance
```{r}
prop_no_vending_entry <- nyc_subway_entrances |> 
  filter (vending == "NO") |> 
  summarize(prop_entry = mean (entry, na.rm = TRUE)) |> 
  pull(prop_entry)
```
The proportion of station entrances/exits without vending that allow entrance is `r prop_no_vending_entry`.

## Section 5 Reformatting data for route analysis
```{r}
nyc_subway2 <- nyc_subway_entrances |> 
  pivot_longer(cols = starts_with("route"),
    names_to = "route_number",
    values_to = "route_name",
    values_drop_na = TRUE) |> 
  distinct(line, station_name, route_name, .keep_all = TRUE)

a_train_stations <- nyc_subway2 |> 
  filter(route_name == "A") |> 
  distinct(line, station_name) |> 
  nrow()

a_train_ada_stations <- nyc_subway2 |> 
  filter(route_name == "A", ada == TRUE) |> 
  distinct(line, station_name) |> 
  nrow()
```
There are `r a_train_stations` distinct stations that serve the A train. Of these, `r a_train_ada_stations` are ADA compliant.


# Problem 2

## Section 1: Import and clean the data 

### For Mr.Trash Wheel
```{r}
mr_trash_wheel <- read_excel("data/202409 Trash Wheel Collection Data.xlsx",
  sheet = "Mr. Trash Wheel",
  range = cell_cols("A:N"),
  skip = 1) |>
  janitor::clean_names() |>
  filter(!is.na(dumpster)) |>
  mutate(sports_balls = as.integer(round(sports_balls)),trash_wheel = "Mr. Trash Wheel")
```

###  For Professor Trash Wheel
```{r}
professor_trash_wheel <- read_excel("data/202409 Trash Wheel Collection Data.xlsx",
  sheet = "Professor Trash Wheel",
  range = cell_cols("A:M"),
  skip = 1) |>
  janitor::clean_names() |>
  filter(!is.na(dumpster)) |>
  mutate(trash_wheel = "Professor Trash Wheel")
```

### For Gwynnda
```{r}
gwynnda <- read_excel("data/202409 Trash Wheel Collection Data.xlsx",
  sheet = "Gwynnda Trash Wheel",
  range = cell_cols("A:L"),
  skip = 1) |>
  janitor::clean_names() |>
  filter(!is.na(dumpster)) |>
  mutate(trash_wheel = "Gwynnda")
```

## Section 2: Data Combination
```{r}
# For Mr. Trash Wheel
mr_trash_wheel <- mr_trash_wheel |>
  mutate(year = as.numeric(year))

# For Professor Trash Wheel
professor_trash_wheel <- professor_trash_wheel |>
  mutate(year = as.numeric(year))

# For Gwynnda
gwynnda <- gwynnda |>
  mutate(year = as.numeric(year))

# Now combine the datasets
trash_wheel_data <- bind_rows(mr_trash_wheel, professor_trash_wheel, gwynnda)
```

## Section 3: Data Description and Analysis
```{r}
# Calculate total weight of trash collected by Professor Trash Wheel
total_weight_professor <- professor_trash_wheel |>
  summarize(total_weight = sum(weight_tons, na.rm = TRUE)) |>
  pull(total_weight)

# Calculate total number of cigarette butts collected by Gwynnda in June 2022
gwynnda_cig_butts_june_2022 <- gwynnda |>
  filter(year == 2022, month == "June") |>
  summarize(total_cig_butts = sum(cigarette_butts, na.rm = TRUE)) |>
  pull(total_cig_butts)

total_weight_professor <- total_weight_professor
gwynnda_cig_butts_june_2022 <- gwynnda_cig_butts_june_2022
```

The combined trash wheel dataset contains `r nrow(trash_wheel_data)` observations across three different trash wheels: Mr. Trash Wheel, Professor Trash Wheel, and Gwynnda. This dataset has `r ncol(trash_wheel_data)` columns. Key variables include dumpster, month, year, date, weight_tons, volume_cubic_yards, and specific types of trash such as plastic_bottles, polystyrene, cigarette_butts, glass_bottles, plastic_bags, and wrappers. Additional variables include homes_powered, which estimates the number of homes that could be powered by the collected trash. The trash_wheel variable distinguishes between the three sources. This comprehensive dataset spans multiple years, offering insights into trash collection efforts in the Baltimore harbor.
Professor Trash Wheel collected a total of `r sprintf("%.2f", total_weight_professor)` tons of trash since its inception. In a specific timeframe, Gwynnda collected `r format(gwynnda_cig_butts_june_2022, big.mark = ",")` cigarette butts in June 2022 alone. 

# Problem 3
## Section 1
### 1.1 Import and clean bakers dataset
I separated the baker_name column into baker_first_name and last_name. This was done to align with other datasets that primarily use first names for identification.
```{r, message=FALSE}
bakers_c <- read_csv("data/bakers.csv", na = c("NA", "", ".", " ")) |> 
  janitor::clean_names() |> 
  separate(baker_name, into = c("baker_first_name", "last_name"), sep = " ")
```

### 1.2 Import and clean bakes dataset
I renamed the baker column to baker_first_name for consistency with other datasets.
```{r, message=FALSE}
bakes_c <- read_csv("data/bakes.csv", na = c("NA", "", ".", " ")) |> 
  janitor::clean_names() |> 
  rename(baker_first_name = baker)
```

### 1.3 Import and clean results dataset
I skipped the first two rows during import as they likely contained header information. Also, I renamed the baker column to baker_first_name for consistency, and used case_match() to standardize and clarify the result column values, making them more descriptive (e.g., "IN" to "stayed in", "OUT" to "Eliminated").
```{r, message=FALSE}
results_c <- read_csv("data/results.csv", na = c("NA", "", ".", " "), skip = 2) |> 
  janitor::clean_names() |> 
  rename(baker_first_name = baker) |> 
  mutate(result = case_match(
      result,
      "IN" ~ "stayed in",
      "OUT" ~ "Eliminated",
      "STAR BAKER" ~ "Star Baker",
      "WINNER" ~ "Series Winner",
      "Runner-up" ~ "Series Runner up",
      "WD" ~ "withdrew"
    )
  )
```

### 1.4 Import and clean results dataset
```{r, message=FALSE}
results_c <- read_csv("data/results.csv", na = c("NA", "", ".", " "), skip = 2) |> 
  janitor::clean_names() |> 
  rename(baker_first_name = baker) |> 
  mutate(result = case_match(
      result,
      "IN" ~ "stayed in",
      "OUT" ~ "Eliminated",
      "STAR BAKER" ~ "Star Baker",
      "WINNER" ~ "Series Winner",
      "Runner-up" ~ "Series Runner up",
      "WD" ~ "withdrew"
    )
  )
```

### 1.5 Check for completeness and correctness across datasets.
```{r}
anti_join(bakers_c, bakes_c)

anti_join(bakers_c, results_c)

anti_join(results_c, bakes_c)
```

### 1.6 Merge datasets
```{r}
final_dataset_merge <- results_c |> 
  left_join(bakers_c, by = c("baker_first_name", "series") ) |> 
  left_join(bakes_c, by = c("baker_first_name", "series", "episode" )) |> 
  relocate(series, episode, baker_first_name)
```

### 1.7 Export Final dataset
```{r}
write_csv(final_dataset_merge, "data/final_dataset_merged.csv")
```

### 1.8 Description
In my data cleaning process, I focused on creating consistency across the datasets. I separated the baker's full name into first and last names in the bakers dataset, as other datasets primarily used first names. This decision helped streamline the merging process later.
For the results dataset, I standardized the 'result' column values to make them more descriptive. For example, I changed "IN" to "stayed in" and "OUT" to "Eliminated".
I used anti_join operations to check for mismatches between datasets, ensuring data integrity.
When merging the datasets, I used left joins starting with the results dataset as the base. I then joined the bakers and bakes data using common columns like baker_first_name, series, and episode.
The resulting merged dataset, final_dataset_merge, contains `r ncol(final_dataset_merge)` variables and `r nrow(final_dataset_merge)` observations.

## Section 2
```{r}
library(dplyr)
library(knitr)
star_bakers_winners <- final_dataset_merge |> 
  filter(series >= 5 & series <= 10 & (result == "Star Baker" | result == "Series Winner")) |> 
  select(series, episode, baker_first_name, result) |> 
  arrange(series, episode)
knitr::kable(star_bakers_winners, caption = "Star Bakers and Winners for Seasons 5-10")
```

This table reveals interesting patterns in the Great British Bake Off, Seasons 5-10. While frequent Star Bakers often become series winners, there are notable exceptions. Nancy in Season 5 won despite Richard's five Star Baker titles. Nadiya and Candice in Seasons 6 and 7 followed a more predictable path, earning multiple Star Baker titles before winning.
The most surprising outcome is in Season 10, where David won without ever being Star Baker. This contrasts with Steph, who was Star Baker four times but didn't win. Rahul's win in Season 9 shows how early success combined with a strong finish can lead to victory.

## Section 3
### 3.1 Import and clean data
```{r, message=FALSE}
viewers_c <- read_csv("data/viewers.csv", na = c("NA", "", ".", " ")) |> 
  janitor::clean_names()
```

### 3.2 Show first 10 rows
```{r}
head(viewers_c,10)
```

### 3.3 Average viewership in Season 1 and Season 5
```{r}
avg_season1 = mean(pull(viewers_c, series_1), na.rm = TRUE)
avg_season5 = mean(pull (viewers_c, series_5), na.rm = TRUE)
```

The analysis of the GBBO viewership data shows that:

1. The average viewership in Season 1 was `r avg_season1`.
2. The average viewership in Season 5 was `r avg_season5`.








